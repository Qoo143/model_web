"""
RAG Chain

æ•´åˆæª¢ç´¢å’Œç”Ÿæˆçš„å®Œæ•´ RAG æµç¨‹
"""

from typing import List, Optional, Dict, Any
from dataclasses import dataclass, field
from datetime import datetime

from app.services.rag.retriever import RetrieverService, retriever_service, RetrievalResult
from app.services.llm.base import BaseLLMService, Message
from app.services.llm.factory import get_llm_service
from app.core.config import settings


@dataclass
class RAGResponse:
    """RAG å›æ‡‰"""
    answer: str
    sources: List[Dict[str, Any]]
    model: str
    retrieval_count: int
    generation_time: Optional[float] = None
    confidence: float = 0.0
    metadata: Dict[str, Any] = field(default_factory=dict)


class RAGChain:
    """
    RAG Chain

    æ¥­å‹™é‚è¼¯ï¼š
    1. æ¥æ”¶ä½¿ç”¨è€…å•é¡Œ
    2. æª¢ç´¢ç›¸é—œæ–‡ä»¶ç‰‡æ®µ
    3. æ§‹å»ºåŒ…å«ä¸Šä¸‹æ–‡çš„ prompt
    4. å‘¼å« LLM ç”Ÿæˆç­”æ¡ˆ
    5. è§£æä¸¦è¿”å›ç­”æ¡ˆå’Œä¾†æº

    Prompt çµæ§‹ï¼š
    - ç³»çµ±æŒ‡ç¤ºï¼šå®šç¾© AI çš„è§’è‰²å’Œè¡Œç‚º
    - ä¸Šä¸‹æ–‡ï¼šæª¢ç´¢åˆ°çš„ç›¸é—œæ–‡ä»¶
    - å°è©±æ­·å²ï¼šä¹‹å‰çš„å°è©±ï¼ˆå¯é¸ï¼‰
    - ä½¿ç”¨è€…å•é¡Œï¼šç•¶å‰å•é¡Œ
    """

    # ç³»çµ±æç¤ºæ¨¡æ¿ (å„ªåŒ–ç‰ˆ)
    SYSTEM_PROMPT_TEMPLATE = """ä½ æ˜¯ä¸€å€‹å°ˆæ¥­çš„çŸ¥è­˜åº«å•ç­”åŠ©æ‰‹ã€‚è«‹æ ¹æ“šæä¾›çš„æ–‡ä»¶å…§å®¹ç²¾ç¢ºå›ç­”å•é¡Œã€‚

## å›ç­”è¦å‰‡

1. **åš´æ ¼ä¾æ“šæ–‡ä»¶**ï¼šåªä½¿ç”¨æä¾›çš„æ–‡ä»¶å…§å®¹å›ç­”ï¼Œçµ•ä¸ç·¨é€ è³‡è¨Š
2. **æ˜ç¢ºå¼•ç”¨ä¾†æº**ï¼šå›ç­”æ™‚æ¨™è¨»è³‡è¨Šä¾†è‡ªå“ªå€‹æ–‡ä»¶ï¼Œæ ¼å¼ç‚º [ä¾†æº: æ–‡ä»¶å]
3. **çµæ§‹åŒ–å›ç­”**ï¼š
   - å…ˆç›´æ¥å›ç­”å•é¡Œçš„æ ¸å¿ƒ
   - å†è£œå……ç›¸é—œç´°ç¯€å’ŒèƒŒæ™¯
   - å¦‚æœ‰å¤šå€‹è¦é»ï¼Œä½¿ç”¨æ¢åˆ—å¼å‘ˆç¾
4. **å¦æ‰¿ä¸çŸ¥**ï¼šå¦‚æœæ–‡ä»¶ä¸­ç¢ºå¯¦æ²’æœ‰ç›¸é—œè³‡è¨Šï¼Œè«‹å›ç­”ã€Œæ ¹æ“šç›®å‰çš„æ–‡ä»¶å…§å®¹ï¼Œå°šç„¡æ³•æ‰¾åˆ°é—œæ–¼æ­¤å•é¡Œçš„è³‡è¨Šã€
5. **èªè¨€åŒ¹é…**ï¼šä½¿ç”¨èˆ‡å•é¡Œç›¸åŒçš„èªè¨€å›ç­”

ç•¶å‰æ™‚é–“ï¼š{current_time}
"""

    # ä¸Šä¸‹æ–‡æ¨¡æ¿ (å„ªåŒ–ç‰ˆ)
    CONTEXT_TEMPLATE = """## åƒè€ƒæ–‡ä»¶å…§å®¹

ä»¥ä¸‹æ˜¯èˆ‡æ‚¨å•é¡Œæœ€ç›¸é—œçš„æ–‡ä»¶ç‰‡æ®µï¼Œè«‹ä»”ç´°é–±è®€å¾Œå›ç­”ï¼š

{contexts}

---
è«‹æ ¹æ“šä»¥ä¸Šå…§å®¹å›ç­”å•é¡Œã€‚
"""

    # å–®å€‹æ–‡ä»¶ç‰‡æ®µæ¨¡æ¿ (å„ªåŒ–ç‰ˆ)
    CHUNK_TEMPLATE = """### ğŸ“„ {filename}
```
{content}
```
"""

    def __init__(
        self,
        retriever: RetrieverService = None,
        llm_service: BaseLLMService = None,
        top_k: int = None
    ):
        """
        åˆå§‹åŒ– RAG Chain

        Args:
            retriever: æª¢ç´¢æœå‹™
            llm_service: LLM æœå‹™
            top_k: æª¢ç´¢æ•¸é‡
        """
        self.retriever = retriever or retriever_service
        self.llm = llm_service or get_llm_service()
        self.top_k = top_k or settings.TOP_K_RETRIEVAL

    async def query(
        self,
        question: str,
        group_id: int,
        document_ids: Optional[List[int]] = None,
        conversation_history: Optional[List[Dict[str, str]]] = None,
        top_k: int = None,
        llm_provider: str = None
    ) -> RAGResponse:
        """
        åŸ·è¡Œ RAG æŸ¥è©¢

        Args:
            question: ä½¿ç”¨è€…å•é¡Œ
            group_id: ç¾¤çµ„ ID
            document_ids: é™åˆ¶åœ¨é€™äº›æ–‡ä»¶ä¸­æœå°‹ï¼ˆå¯é¸ï¼‰
            conversation_history: å°è©±æ­·å²ï¼ˆå¯é¸ï¼‰
            top_k: æª¢ç´¢æ•¸é‡ï¼ˆè¦†è“‹é è¨­å€¼ï¼‰
            llm_provider: LLM æä¾›è€…ï¼ˆè¦†è“‹é è¨­å€¼ï¼‰

        Returns:
            RAGResponse: RAG å›æ‡‰
        """
        k = top_k or self.top_k
        retrieval_results = []

        # 1. å˜—è©¦æª¢ç´¢ç›¸é—œæ–‡ä»¶
        try:
            if document_ids:
                retrieval_results = await self.retriever.retrieve_for_documents(
                    query=question,
                    document_ids=document_ids,
                    top_k=k
                )
            else:
                retrieval_results = await self.retriever.retrieve_for_group(
                    query=question,
                    group_id=group_id,
                    top_k=k
                )
        except Exception as e:
            # å¦‚æœæª¢ç´¢å¤±æ•—ï¼ˆå¦‚ Ollama æœªå•Ÿå‹•ï¼‰ï¼Œç›´æ¥ä½¿ç”¨ LLM
            import logging
            logging.warning(f"RAG retrieval failed, using direct LLM: {e}")
            retrieval_results = []

        # 2. æ§‹å»ºä¸Šä¸‹æ–‡
        context = self._build_context(retrieval_results)

        # 3. æ§‹å»ºè¨Šæ¯
        messages = self._build_messages(
            question=question,
            context=context,
            conversation_history=conversation_history
        )

        # 4. é¸æ“‡ LLM
        llm = get_llm_service(llm_provider) if llm_provider else self.llm

        # 5. å‘¼å« LLM
        llm_response = await llm.chat(messages)

        # 6. æ§‹å»ºä¾†æºè³‡è¨Š
        sources = [
            {
                "document_id": r.document_id,
                "document_name": r.document_name,
                "chunk_index": r.chunk_index,
                "content": r.content[:200] + "..." if len(r.content) > 200 else r.content,
                "score": round(r.score, 3)
            }
            for r in retrieval_results
        ]

        # 7. è¨ˆç®—ä¿¡å¿ƒåˆ†æ•¸ï¼ˆåŸºæ–¼æª¢ç´¢åˆ†æ•¸ï¼‰
        confidence = 0.0
        if retrieval_results:
            confidence = sum(r.score for r in retrieval_results) / len(retrieval_results)

        return RAGResponse(
            answer=llm_response.content,
            sources=sources,
            model=llm_response.model,
            retrieval_count=len(retrieval_results),
            generation_time=llm_response.generation_time,
            confidence=round(confidence, 3),
            metadata={
                "prompt_tokens": llm_response.prompt_tokens,
                "completion_tokens": llm_response.completion_tokens,
                "total_tokens": llm_response.total_tokens
            }
        )

    def _build_context(self, retrieval_results: List[RetrievalResult]) -> str:
        """æ§‹å»ºä¸Šä¸‹æ–‡æ–‡æœ¬"""
        if not retrieval_results:
            return ""

        chunks = []
        for r in retrieval_results:
            chunk_text = self.CHUNK_TEMPLATE.format(
                filename=r.document_name or f"Document {r.document_id}",
                content=r.content
            )
            chunks.append(chunk_text)

        contexts = "\n".join(chunks)
        return self.CONTEXT_TEMPLATE.format(contexts=contexts)

    def _build_messages(
        self,
        question: str,
        context: str,
        conversation_history: Optional[List[Dict[str, str]]] = None
    ) -> List[Message]:
        """æ§‹å»º LLM è¨Šæ¯"""
        messages = []

        # ç³»çµ±æç¤º
        system_prompt = self.SYSTEM_PROMPT_TEMPLATE.format(
            current_time=datetime.now().strftime("%Y-%m-%d %H:%M")
        )
        messages.append(Message(role="system", content=system_prompt))

        # ä¸Šä¸‹æ–‡
        if context:
            messages.append(Message(role="user", content=context))
            messages.append(Message(role="assistant", content="æˆ‘å·²é–±è®€ä»¥ä¸Šæ–‡ä»¶å…§å®¹ï¼Œè«‹æå‡ºæ‚¨çš„å•é¡Œã€‚"))

        # å°è©±æ­·å²
        if conversation_history:
            for msg in conversation_history[-6:]:  # æœ€å¤šä¿ç•™æœ€è¿‘ 3 è¼ªå°è©±
                messages.append(Message(
                    role=msg["role"],
                    content=msg["content"]
                ))

        # ç•¶å‰å•é¡Œ
        messages.append(Message(role="user", content=question))

        return messages

    async def query_stream(
        self,
        question: str,
        group_id: int,
        document_ids: Optional[List[int]] = None,
        conversation_history: Optional[List[Dict[str, str]]] = None,
        top_k: int = None,
        llm_provider: str = None
    ):
        """
        ä¸²æµå¼ RAG æŸ¥è©¢

        èˆ‡ query() ç›¸åŒï¼Œä½†ä¸²æµè¿”å›ç­”æ¡ˆ
        ç”¨æ–¼å³æ™‚é¡¯ç¤ºç”Ÿæˆå…§å®¹
        """
        k = top_k or self.top_k

        # 1-3: èˆ‡ query() ç›¸åŒ
        if document_ids:
            retrieval_results = await self.retriever.retrieve_for_documents(
                query=question,
                document_ids=document_ids,
                top_k=k
            )
        else:
            retrieval_results = await self.retriever.retrieve_for_group(
                query=question,
                group_id=group_id,
                top_k=k
            )

        context = self._build_context(retrieval_results)
        messages = self._build_messages(question, context, conversation_history)

        # 4. é¸æ“‡ LLM ä¸¦ä¸²æµç”Ÿæˆ
        llm = get_llm_service(llm_provider) if llm_provider else self.llm

        # æ§‹å»ºå®Œæ•´ prompt
        full_prompt = "\n".join([m.content for m in messages if m.role == "user"])
        system_prompt = next((m.content for m in messages if m.role == "system"), None)

        # ä¸²æµè¿”å›
        async for chunk in llm.stream(full_prompt, system_prompt):
            yield chunk

        # è¿”å›ä¾†æºè³‡è¨Šï¼ˆä½œç‚ºæœ€çµ‚è¨Šæ¯ï¼‰
        sources = [
            {
                "document_id": r.document_id,
                "document_name": r.document_name,
                "score": round(r.score, 3)
            }
            for r in retrieval_results
        ]
        yield {"type": "sources", "data": sources}


# å–®ä¾‹å¯¦ä¾‹
rag_chain = RAGChain()
